---
title: "feature-selection"
author: "Jin"
date: "2021年5月14日"
output: html_document
---

- Set working directory
```{r setup}
your.wd <- "/home/jinyunfan/Documents/bioinfo/exRNA/ez-prediction"
knitr::opts_knit$set(root.dir = your.wd)
```

- Select relevant features (significantly differentially expressed genes from training set)

- Load training data
```{R}
sample.info <- read.table("metadata/dataset.split.txt", header = T, sep ="\t",row.names=1,stringsAsFactors = F,check.names = F)
count.path <- "data/TCGA-CRC-counts.txt"
count.matrix <- read.table(count.path, header = T, sep ="\t",row.names=1,stringsAsFactors = F,check.names = F)
count.matrix <- as.matrix(count.matrix)
train.ids <- rownames(sample.info)[sample.info$dataset=="train"]
count.matrix <- count.matrix[,train.ids]
labels <- sample.info[train.ids,"tissue.types"]
labels <- factor(labels,level=c("normal","tumor"))
```

- Perform differential expression
```{R}
library(edgeR)
y <- edgeR::DGEList(counts=count.matrix)
keep <- edgeR::filterByExpr(y,group = labels)
y <- y[keep, , keep.lib.sizes=FALSE]
y <- edgeR::calcNormFactors(y,method="TMM")

design <- model.matrix(~labels)
y <- estimateDisp(y, design)
fit.ql <- glmQLFit(y, design)
tumor.vs.normal.ql <- glmQLFTest(fit.ql, coef=2)
tumor.vs.normal.de <- topTags(tumor.vs.normal.ql,n=Inf)
write.table(tumor.vs.normal.de,"output/features/de.train.txt",sep="\t",quote=FALSE)
```

- Instead of differential expression, you can use random forest or other model to perform feature selection


- Use `rfcv` in `randomForest` package to show how perform change as number of features descrease
- We genrally use cross validation for parameter tuning, but in `randomForest` package, the `rfcv` function is used to determine the relationship between number of features and model performance ...  
```{R}
library(randomForest)
exp.mat <- read.table("output/processed/log.cpm.scaled.txt", header = T, sep ="\t",row.names=1,stringsAsFactors = F,check.names = F)
exp.mat <- t(as.matrix(exp.mat)[,train.ids])
cv.result <- randomForest::rfcv(exp.mat,labels)
plot(cv.result$n.var, cv.result$error.cv, log="x", type="o", lwd=2)
```

- Random forest is random, do not expect it to give the same result if you run the second time
- If you want the result to be reproducible, add `set.seed(...)`

```{R}
set.seed(666)
rf.model <- randomForest::randomForest(exp.mat,labels)
feature.importance <- randomForest::importance(rf.model)
feature.names <- rownames(feature.importance)
top.50.idx <- order(feature.importance,decreasing = T)[1:50]
top.50.important <- feature.importance[top.50.idx]
names(top.50.important) <- feature.names[top.50.idx]
head(top.50.important)
```


- We can do similar things with SVM, gradient boosting, logistic regression, and many other models, just like random forest. But personally thinking, differential expression is enough ...


### relief base feature selection
```{R}
library(FSinR)
evaluator <- filterEvaluator('ReliefFeatureSetMeasure')
get.K.best <- selectKBest(k=6)
## Only take 50 features as a demo input
exp.data <- as.data.frame(exp.mat[,1:50])
exp.data[["labels"]] <- labels
results <- get.K.best(exp.data, "labels", evaluator)
results$featuresSelected
```

- Perform RFE with caret

```{R}
library(caret)
# use ROC as metric instead of accuracy by default
rfFuncs$summary <- twoClassSummary 
rfectrl <- rfeControl(functions=rfFuncs,
                      verbose = TRUE,
                      method="repeatedcv",
                      number=5,repeats=10)
# Use 5 fold cross validation, repeat 10 times

# Select feature set of given sizes 
rfe.results <- rfe(exp.mat,labels, 
               sizes=c(1,2,4,8,16,32), 
               rfeControl=rfectrl,
               metric = "ROC")
predictors(rfe.results)
rfe.results$results
```



- You can perform exhaustive search with cross-validation for feature selection
- Some one thought this is likely  to lead to overfitting, see <https://stats.stackexchange.com/questions/38038/can-i-perform-an-exhaustive-search-with-cross-validation-for-feature-selection>
- It will be a large difference if you change the random seed. But after all there is still some correlation...

```{R}
library(caret)
library(pROC)
## Suppose we some how reduce the search tp 8 features, we want to 6 most important ones
## features.started is 8 features we start from
features.started <- tail(rownames(tumor.vs.normal.de$table),8)
all.combinations <- combn(1:8, 6, simplify = FALSE)
n.combinations <- length(all.combinations)
exp.mat.started <- exp.mat[,features.started]

cv.for.FS <- function(x,y){
    #set.seed(666)
    rf.cv <- train(x,y,
                 method = "rf",
                 preProcess = NULL,
                 ntree = 500,
                 trControl = trainControl(method = "LOOCV",savePredictions = T,classProbs = T))
   roc.curve <- roc(rf.cv$pred$obs,rf.cv$pred$tumor)
  # See https://stats.stackexchange.com/questions/386326/appropriate-way-to-get-cross-validated-auc
  return(as.numeric(roc.curve$auc))
}

#cv.aurocs <- lapply(all.combinations,function(indices,x,y){cv.for.FS(x[,indices],y)},x=exp.mat.started,y=labels)

# A parallel version, should be faster
library(parallel)
cv.aurocs <- mclapply(all.combinations,function(indices,x,y){cv.for.FS(x[,indices],y)},x=exp.mat.started,y=labels,mc.cores=4) 

all.combinations[which.max(unlist(cv.aurocs))]

```