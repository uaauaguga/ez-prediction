---
title: "feature-selection"
author: "Jin"
date: "2021年5月14日"
output: html_document
---

- Set working directory
```{r setup}
your.wd <- "/home/jinyunfan/Documents/bioinfo/exRNA/ez-prediction"
knitr::opts_knit$set(root.dir = your.wd)
```

- Select relevant features (significantly differentially expressed genes from training set)

- Load training data
```{R}
sample.info <- read.table("metadata/dataset.split.txt", header = T, sep ="\t",row.names=1,stringsAsFactors = F,check.names = F)
count.path <- "data/TCGA-CRC-counts.txt"
count.matrix <- read.table(count.path, header = T, sep ="\t",row.names=1,stringsAsFactors = F,check.names = F)
count.matrix <- as.matrix(count.matrix)
train.ids <- rownames(sample.info)[sample.info$dataset=="train"]
count.matrix <- count.matrix[,train.ids]
labels <- sample.info[train.ids,"tissue.types"]
labels <- factor(labels,level=c("normal","tumor"))
```

- Perform differential expression
```{R}
library(edgeR)
y <- edgeR::DGEList(counts=count.matrix)
keep <- edgeR::filterByExpr(y,group = labels)
y <- y[keep, , keep.lib.sizes=FALSE]
y <- edgeR::calcNormFactors(y,method="TMM")

design <- model.matrix(~labels)
y <- estimateDisp(y, design)
fit.ql <- glmQLFit(y, design)
tumor.vs.normal.ql <- glmQLFTest(fit.ql, coef=2)
tumor.vs.normal.de.top50 <- topTags(tumor.vs.normal.ql,n=Inf)
write.table(tumor.vs.normal.de.top50,"output/features/de.train.txt",sep="\t",quote=FALSE)
```

- Instead of differential expression, you can use random forest or other model to perform feature selection


- Use `rfcv` in `randomForest` package to show how perform change as number of features descrease
- We genrally use cross validation for parameter tuning, but in `randomForest` package, the `rfcv` function is used to determine the relationship between number of features and model performance ...  
```{R}
library(randomForest)
exp.mat <- read.table("output/processed/log.cpm.scaled.txt", header = T, sep ="\t",row.names=1,stringsAsFactors = F,check.names = F)
exp.mat <- t(as.matrix(exp.mat)[,train.ids])
cv.result <- randomForest::rfcv(exp.mat,labels)
plot(cv.result$n.var, cv.result$error.cv, log="x", type="o", lwd=2)
```

- Take 50 most important features (although seems 10 feature is enough)
- Random forest is random, do not expect it to give the same result if you run the second time
- If you want the result to be reproducible, add `set.seed(...)`

```{R}
set.seed(666)
rf.model <- randomForest::randomForest(exp.mat,labels)
feature.importance <- randomForest::importance(rf.model)
feature.names <- rownames(feature.importance)
top.50.idx <- order(feature.importance,decreasing = T)[1:50]
top.50.important <- feature.importance[top.50.idx]
names(top.50.important) <- feature.names[top.50.idx]
head(top.50.important)
```


- We can do similar things with SVM, gradient boosting, logistic regression, and many other models, just like random forest. But personally thinking, differential expression is enough ...