---
title: "performance"
author: "Jin"
date: "2021年5月14日"
output: html_document
---

- Set working directory
```{r setup}
your.wd <- "/home/jinyunfan/Documents/bioinfo/exRNA/ez-prediction"
knitr::opts_knit$set(root.dir = your.wd)
```

## Prepare training and testing data


- The model performance should be perfect, as tumor and tumor adjancent tissue are highly distinct
- To illustrate practical cases, we take most insignificant features as model input (in theory we should take most significant ones)


```{R}
sample.info <- read.table("metadata/dataset.split.txt", header = T, sep ="\t",row.names=1,check.names = F)
exp.mat <- read.table("output/processed/log.cpm.scaled.txt", header = T, sep ="\t",row.names=1,stringsAsFactors = F,check.names = F)
train.ids <- rownames(sample.info)[sample.info$dataset=="train"]
test.ids <- rownames(sample.info)[sample.info$dataset=="test"]
features.de <- read.table("output/features/de.train.txt", header = T, sep ="\t",row.names=1,stringsAsFactors = F,check.names = F)

feature.ids <- rownames(features.de)
set.seed(666)
feature.ids <-  c(sample(feature.ids,6,replace=F),tail(feature.ids,8))
# You can use the most significant features
exp.mat <- as.matrix(t(exp.mat[feature.ids,]))
train.exp.mat <- exp.mat[train.ids,]
test.exp.mat <- exp.mat[test.ids,]

# If you set stringsAsFactors = F in read.table function, the response variable in `sample.info` is not of factor type
train.labels <- sample.info[train.ids,"tissue.types"]
test.labels <- sample.info[test.ids,"tissue.types"]
```



```{R}
library(glmnet)
library(e1071)
library(randomForest)
library(pROC)
library(caret)
library(ggplot2)
```

## Logistic regression
- Logistic regression for cross validation and performance evaluation
- Hyperparameter
  - alpha: default is 1, the lasso penalty. If set to 0, corresponds to ridge regression. If between 0 and 1, corresponds to elastic net
  - lambda: the stringency of regularization
- `glmnet`
```{R}
tuneGrid = expand.grid(alpha = 0,lambda = seq(0,0.5,length=10))
trControl = trainControl(method = "LOOCV",savePredictions = T,classProbs = T,summaryFunction = twoClassSummary)
glmnet.cross.validation <- train(train.exp.mat,train.labels,
                 method = "glmnet",
                 family="binomial",
                 metric = "ROC",
                 tuneGrid = tuneGrid,
                 preProcess = NULL,
                 trControl = trControl)

cv.performance <- glmnet.cross.validation$results[order(glmnet.cross.validation$results$ROC,decreasing = T),]
cv.performance[order(cv.performance$ROC,decreasing = T)[1:10],]
```

- Evaluate performance
  - Calculate AUROC with `ci.auc`
  - Calculate best `recall` and `precision` with `ci.coords`
  
```{R}
glmnet.fit <- glmnet(train.exp.mat,train.labels,lambda = glmnet.cross.validation$bestTune$lambda,alpha = 1,family = "binomial")
pred.proba <- predict(glmnet.fit,newx=test.exp.mat,type="response") # type="response" makes it return possibility
roc.curve <- roc(test.labels,as.numeric(pred.proba))
ci.auc(roc.curve,conf.level = 0.95)
plot(1-roc.curve$specificities,roc.curve$sensitivities,type="l")
# best.method=c("youden", "closest.topleft")
# youden by default. 
ci.coords(roc.curve,x="best",conf.level = 0.95,ret = c("recall","specificity","precision"),best.method="youden",best.policy="random")
```


- We can do exactly same things with SVM, GBDT, and random forest

## SVM
- SVM, use package `e1071`
- Default parameters usually works well
- If perform cross validation, tune hyper-parameter `gamma` and `cost`

```{R}
# search space of gamma: 2^(-1:1)/50, 1/(data dimension) by default 
# search space of cost: 2^(-1:1), 1 by default
gamma <- 2^(-2:2)/50
cost <- 2^(-2:2)
cv.svm.results <- tune.svm(train.exp.mat,train.labels,gamma = gamma, cost = cost ,probability = TRUE)
cv.svm.results$best.parameters
# Perform prediction using best model
pred.proba <- predict(cv.svm.results$best.model,newdata=test.exp.mat,probability = TRUE)
pred.proba <- attr(pred.proba,"probabilities")[,"tumor"]
# Evaluate performance
roc.curve <- roc(test.labels,as.numeric(pred.proba))
ci.auc(roc.curve,conf.level = 0.95)
plot(1-roc.curve$specificities,roc.curve$sensitivities,type="l")
ci.coords(roc.curve,x="best",conf.level = 0.95,ret = c("recall","specificity","precision"),best.policy="random")
```

## Random Forest
- Use package `randomForest`
- You don't need to tune number of trees, but may tune the `mtry` parameter

```{R}
trControl = trainControl(method = "LOOCV",savePredictions = T,classProbs = T,summaryFunction = twoClassSummary)
rf.cross.validation <- train(train.exp.mat,train.labels,
                 method = "rf",
                 metric = "ROC",
                 preProcess = NULL,
                 trControl = trControl)

cv.performance <- rf.cross.validation$results[order(rf.cross.validation$results$ROC,decreasing = T),]
cv.performance[order(cv.performance$ROC,decreasing = T),]
```

- You can also use built in function `tuneRF` in `RandomForest` package, which use OOB error for parameter tuning

```{R}
tuneRF(train.exp.mat,train.labels)
```


```{R}
rf.model.fitted <- randomForest(train.exp.mat,train.labels,mtry = rf.cross.validation$bestTune$mtry)
pred.proba <- predict(rf.model.fitted,newdata=test.exp.mat,type="prob")
pred.proba <- pred.proba[,"tumor"]
roc.curve <- roc(test.labels,as.numeric(pred.proba))
ci.auc(roc.curve,conf.level = 0.95)
plot(1-roc.curve$specificities,roc.curve$sensitivities,type="l")
ci.coords(roc.curve,x="best",conf.level = 0.95,ret = c("recall","precision"),best.policy="random")
```

- You can also manually implement some cross validation 

```{R}
training.indices <- createDataPartition(sample.info$tissue.types,times = 10,p = 0.8)
aurocs <- c()
for(i in seq(length(training.indices))){
  train.ids <- training.indices[[i]]
  rf.model.fitted <- randomForest(exp.mat[train.ids,],sample.info[train.ids,"tissue.types"])
  pred.proba <- predict(rf.model.fitted,newdata=exp.mat[-train.ids,],type="prob")
  pred.proba <- pred.proba[,"tumor"]
  roc.curve <- roc(sample.info$tissue.types[-train.ids],as.numeric(pred.proba))
  aurocs <- c(aurocs,auc(roc.curve))
}
aurocs
```
