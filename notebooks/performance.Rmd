---
title: "performance"
author: "Jin"
date: "2021年5月14日"
output: html_document
---

- Set working directory
```{r setup}
your.wd <- "/home/jinyunfan/Documents/bioinfo/exRNA/ez-prediction"
knitr::opts_knit$set(root.dir = your.wd)
```

## Prepare training and testing data


- The model performance should be perfect, as tumor and tumor adjancent tissue are highly distinct
- To illustrate practical cases, we take most insignificant features as model input (in theory we should take most significant ones)


```{R}
sample.info <- read.table("metadata/dataset.split.txt", header = T, sep ="\t",row.names=1,stringsAsFactors = F,check.names = F)
exp.mat <- read.table("output/processed/log.cpm.scaled.txt", header = T, sep ="\t",row.names=1,stringsAsFactors = F,check.names = F)
train.ids <- rownames(sample.info)[sample.info$dataset=="train"]
test.ids <- rownames(sample.info)[sample.info$dataset=="test"]
selected.features <- read.table("output/features/de.train.txt", header = T, sep ="\t",row.names=1,stringsAsFactors = F,check.names = F)

feature.ids <- rownames(selected.features)
n.features <- length(feature.ids)
feature.ids <-  tail(feature.ids,30) #take most insignificant features 
# If you want to use significant ones, simply replace tail(feature.ids,20) with head(feature.ids,30) 
```


```{R}
exp.mat <- as.matrix(t(exp.mat[feature.ids,]))
train.exp.mat <- exp.mat[train.ids,]
test.exp.mat <- exp.mat[test.ids,]

# If you set stringsAsFactors = F in read.table function, the response variable in `sample.info` is not of factor type
sample.info$tissue.types = factor(sample.info$tissue.types,level=c("normal","tumor"))
train.labels <- sample.info[train.ids,"tissue.types"]
test.labels <- sample.info[test.ids,"tissue.types"]
```


```{R}
library(glmnet)
library(pROC)
```

## Logistic regression
- Logistic regression for cross validation and performance evaluation
- Cross validation: tune the regularize factor lambda 
- `glmnet`
```{R}
# Cross validation
cv.lr.result <- cv.glmnet(train.exp.mat,train.labels,family="binomial",nfolds = length(train.labels))
cv.lr.result$lambda.min
# Plot relation between loss and hyper-parameter lambda 
#plot(cv.lr.result$lambda,cv.lr.result$cvm)
pred.proba <- predict(cv.lr.result$glmnet.fit,newx=test.exp.mat,type="response", s =cv.lr.result$lambda.min)
```
- Evaluate performance
  - Calculate AUROC with `ci.auc`
  - Calculate best `recall` and `precision` with `ci.coords`
  
```{R}
roc.curve <- roc(test.labels,as.numeric(pred.proba))
ci.auc(roc.curve,x="best",conf.level = 0.95)
plot(1-roc.curve$specificities,roc.curve$sensitivities,type="l")
# best.method=c("youden", "closest.topleft")
# youden by default. 
ci.coords(roc.curve,x="best",conf.level = 0.95,ret = c("recall","precision"),best.method="youden",best.policy="random")
```


- We can do exactly same things with SVM, GBDT, and random forest

## SVM
- SVM, use package `e1071`
- Default parameters usually works well
- If perform cross validation, tune hyper-parameter `gamma` and `cost`
```{R}
library(e1071)
# search space of gamma: 2^(-1:1)/50, 1/(data dimension) by default 
# search space of cost: 2^(-1:1), 1 by default
gamma <- 2^(-2:2)/50
cost <- 2^(-2:2)
cv.svm.results <- tune.svm(train.exp.mat,train.labels,gamma = gamma, cost = cost ,probability = TRUE)
cv.svm.results$best.parameters
# Perform prediction using best model
pred.proba <- predict(cv.svm.results$best.model,newdata=test.exp.mat,probability = TRUE)
pred.proba <- attr(pred.proba,"probabilities")[,"tumor"]
# Evaluate performance
roc.curve <- roc(test.labels,as.numeric(pred.proba))
ci.auc(roc.curve,x="best",conf.level = 0.95)
plot(1-roc.curve$specificities,roc.curve$sensitivities,type="l")
ci.coords(roc.curve,x="best",conf.level = 0.95,ret = c("recall","precision"),best.policy="random")
```

## Random Forest
- Use package `randomForest`
```{R}
library(randomForest)
rf.model.fitted <- randomForest(train.exp.mat,train.labels)
pred.proba <- predict(rf.model.fitted,newdata=test.exp.mat,type="prob")
pred.proba <- pred.proba[,"tumor"]
roc.curve <- roc(test.labels,as.numeric(pred.proba))
ci.auc(roc.curve,x="best",conf.level = 0.95)
plot(1-roc.curve$specificities,roc.curve$sensitivities,type="l")
ci.coords(roc.curve,x="best",conf.level = 0.95,ret = c("recall","precision"),best.policy="random")
```

### Cross validation without parameter tuning
- Sometimes your may want to perform cross validation, only for performance evaluation, instead of parameter tuning
- For example, shuffle split cross validation like in sklearn's [StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)

```{R}
library(caret)
library(randomForest)
library(pROC)
training.indices <- createDataPartition(sample.info$tissue.types,times = 10,p = 0.8)
aurocs <- c()
for(i in seq(length(training.indices))){
  train.ids <- training.indices[[i]]
  rf.model.fitted <- randomForest(exp.mat[train.ids,],sample.info[train.ids,"tissue.types"])
  pred.proba <- predict(rf.model.fitted,newdata=exp.mat[-train.ids,],type="prob")
  pred.proba <- pred.proba[,"tumor"]
  roc.curve <- roc(sample.info$tissue.types[-train.ids],as.numeric(pred.proba))
  aurocs <- c(aurocs,auc(roc.curve))
}
aurocs
```



